# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

import os
import time
from datetime import timedelta

import torch

from torch.distributed.elastic.multiprocessing.errors import record

from torchtitan import utils
from torchtitan.checkpoint import CheckpointManager, TrainState
from torchtitan.config_manager import JobConfig
from torchtitan.datasets import (
    build_experimental_data_loader,
    build_hf_data_loader,
    build_sft_data_loader,
    build_tokenizer,
)
from torchtitan.float8 import Float8Handler
from torchtitan.logging import init_logger, logger
from torchtitan.metrics import build_device_memory_monitor, build_metric_logger
from torchtitan.models import model_name_to_cls, model_name_to_tokenizer, models_config
from torchtitan.optimizer import build_lr_schedulers, build_optimizers
from torchtitan.parallelisms import (
    models_parallelize_fns,
    models_pipelining_fns,
    ParallelDims,
)
from torchtitan.profiling import maybe_enable_memory_snapshot, maybe_enable_profiling
from torchtitan.utils import device_module, device_type

from transformers import AutoTokenizer

from torchao.float8.fsdp_utils import WeightWithDynamicFloat8CastTensor
torch.serialization.add_safe_globals([WeightWithDynamicFloat8CastTensor])

# Enable debug tracing on failure: https://pytorch.org/docs/stable/elastic/errors.html
@record
def main(job_config: JobConfig):
    init_logger()
    logger.info(f"Starting job: {job_config.job.description}")

    # used for colorful printing
    color = utils.Color if job_config.metrics.enable_color_printing else utils.NoColor

    # take control of garbage collection to avoid stragglers
    gc_handler = utils.GarbageCollection(gc_freq=job_config.training.gc_freq)

    # set determinisism, use seed == None to skip deterministic training
    utils.set_determinism(job_config.training.seed)
    if job_config.training.seed is None:
        logger.info("Deterministic training off")
    else:
        logger.info(
            f"Deterministic training on. Using seed: {job_config.training.seed}"
        )

    # init distributed
    world_size = int(os.environ["WORLD_SIZE"])
    parallel_dims = ParallelDims(
        dp_shard=job_config.training.data_parallel_shard_degree,
        dp_replicate=job_config.training.data_parallel_replicate_degree,
        cp=job_config.experimental.context_parallel_degree,
        tp=job_config.training.tensor_parallel_degree,
        pp=job_config.experimental.pipeline_parallel_degree,
        world_size=world_size,
        enable_loss_parallel=job_config.training.enable_loss_parallel,
    )
    device = torch.device(f"{device_type}:{int(os.environ['LOCAL_RANK'])}")
    device_module.set_device(device)
    utils.init_distributed(job_config)
    # make logger rank0 only
    if torch.distributed.get_rank() != 0:
        logger.setLevel("ERROR")
    # initialize device memory monitor and get peak flops for MFU calculation
    device_memory_monitor = build_device_memory_monitor()
    gpu_peak_flops = utils.get_peak_flops(device_memory_monitor.device_name)
    logger.info(f"Peak FLOPS used for computing MFU: {gpu_peak_flops:.3e}")

    # build meshes
    world_mesh = parallel_dims.build_mesh(device_type=device_type)
    if parallel_dims.dp_enabled:
        dp_mesh = world_mesh["dp"]
        dp_degree, dp_rank = dp_mesh.size(), dp_mesh.get_local_rank()
    else:
        dp_degree, dp_rank = 1, 0

    if parallel_dims.cp_enabled:
        cp_mesh = world_mesh["cp"]
        cp_degree, cp_rank = cp_mesh.size(), cp_mesh.get_local_rank()
    else:
        cp_degree, cp_rank = 1, 0

    if parallel_dims.pp_enabled:
        pp_mesh = world_mesh["pp"]

    model_name = job_config.model.name

    # build dataloader
    if job_config.dataset.use_sft_dataloader:
        tokenizer = AutoTokenizer.from_pretrained(job_config.model.tokenizer_path)
        data_loader = build_sft_data_loader(
            datasets=job_config.dataset.datasets,
            dataset_weights=job_config.dataset.dataset_weights,
            dp_rank=dp_rank,
            dp_degree=dp_degree,
            cp_rank=cp_rank,
            cp_degree=cp_degree,
            batch_size=job_config.training.batch_size,
            seq_len=job_config.training.seq_len,
            naive_padding_free=job_config.dataset.naive_padding_free,
            max_out_tokens=job_config.dataset.max_out_tokens,
        )
    elif job_config.dataset.use_experimental_dataloader:
        tokenizer = AutoTokenizer.from_pretrained(job_config.model.tokenizer_path)
        data_loader = build_experimental_data_loader(
            job_config,
            dp_rank,
            dp_degree,
            None if job_config.dataset.file_type=="arrow" else tokenizer,
        )
    else:
        tokenizer_type = model_name_to_tokenizer[model_name]
        tokenizer = build_tokenizer(tokenizer_type, job_config.model.tokenizer_path)
        data_loader = build_hf_data_loader(
            job_config.training.dataset,
            job_config.training.dataset_path,
            tokenizer,
            job_config.training.batch_size,
            job_config.training.seq_len,
            dp_degree,
            dp_rank,
        )

    # build model (using meta init)
    model_cls = model_name_to_cls[model_name]
    model_config = models_config[model_name][job_config.model.flavor]
    # set the model configs from training inputs:
    # 1. norm type to decide which norm layer to use
    # 2. vocab size from tokenizer
    # 3. max_seq_len base on inputs
    model_config.norm_type = job_config.model.norm_type
    model_config.vocab_size = (
        len(tokenizer.vocab)
        if (
            job_config.dataset.use_experimental_dataloader
            or job_config.dataset.use_sft_dataloader
        )
        else tokenizer.n_words
    )
    model_config.max_seq_len = job_config.training.seq_len

    logger.info(f"Building {model_name} {job_config.model.flavor} with {model_config}")
    with torch.device("meta"):
        model = model_cls.from_model_args(model_config)

    # a no-op hander if float8 is not enabled
    float8_handler = Float8Handler(job_config, parallel_dims)
    # swap to Float8Linear based on float8 configs
    float8_handler.convert_to_float8_training(model)

    # log model size
    model_param_count = utils.get_num_params(model)
    num_flop_per_token = utils.get_num_flop_per_token(
        utils.get_num_params(model, exclude_embedding=True),
        model_config,
        job_config.training.seq_len,
    )
    logger.info(
        f"{color.blue}Model {model_name} {job_config.model.flavor} "
        f"{color.red}size: {model_param_count:,} total parameters{color.reset}"
    )

    # loss function to be shared by Pipeline Parallel and SPMD training
    def loss_fn(pred, labels):
        return torch.nn.functional.cross_entropy(
            pred.flatten(0, 1).float(),
            labels.flatten(0, 1),
            reduction="sum" if job_config.training.sum_loss else "mean",
        )

    if job_config.training.compile:
        loss_fn = torch.compile(loss_fn)

    # move sharded model to CPU/GPU and initialize weights via DTensor
    if job_config.checkpoint.create_seed_checkpoint:
        init_device = "cpu"
        buffer_device = None
    elif job_config.training.enable_cpu_offload:
        init_device = "cpu"
        buffer_device = device_type
    else:
        init_device = device_type
        buffer_device = None

    # apply parallelisms and initialization
    if parallel_dims.pp_enabled:
        # apply PT-D Pipeline Parallel
        pp_schedule, model_parts = models_pipelining_fns[model_name](
            model, pp_mesh, parallel_dims, job_config, device, model_config, loss_fn
        )

        # For PP with looped schedules, each item in model_parts is one stage-model-chunk.
        # We need to iterate through model_parts to apply SPMD parallelisms, compilation,
        # optimizer, and checkpointing
        for m in model_parts:
            # apply SPMD-style PT-D techniques
            models_parallelize_fns[model_name](m, world_mesh, parallel_dims, job_config)
            m.to_empty(device=init_device)
            m.init_weights(buffer_device=buffer_device)
            m.train()
    else:
        # apply PT-D Tensor Parallel, activation checkpointing, torch.compile, Data Parallel
        models_parallelize_fns[model_name](model, world_mesh, parallel_dims, job_config)
        model.to_empty(device=init_device)
        model.init_weights(buffer_device=buffer_device)
        model.train()

        model_parts = [model]

    # TODO: @goon - DELETE
    if job_config.training.debug:
        logger.info(f"{model=}")
        logger.info(f"{model.tok_embeddings.weight.shape=}")
        if "dp_cp" in world_mesh.mesh_dim_names:
            logger.info(f"{world_mesh['dp_cp']=}")
        logger.info(f"{parallel_dims=}")
        logger.info(f"{dp_degree=}, {cp_degree=}")
        logger.info(f"{parallel_dims.dp_shard_enabled=}")
        logger.info(f"{parallel_dims.cp_enabled=}")

    device_mem_stats = device_memory_monitor.get_peak_stats()
    logger.info(
        f"{device_type.upper()} memory usage for model: "
        f"{device_mem_stats.max_reserved_gib:.2f}GiB"
        f"({device_mem_stats.max_reserved_pct:.2f}%)"
    )

    # build optimizer after applying parallelisms to the model
    optimizers = build_optimizers(model_parts, job_config)
    lr_schedulers = build_lr_schedulers(optimizers.optimizers, job_config)

    train_state = TrainState()

    # load initial checkpoint
    checkpoint = CheckpointManager(
        dataloader=data_loader,
        model_parts=model_parts,
        optimizers=optimizers.optimizers,
        lr_schedulers=lr_schedulers.schedulers,
        states={"train_state": train_state},
        job_config=job_config,
    )

    if job_config.checkpoint.create_seed_checkpoint:
        assert (
            world_size == 1
        ), "Must create seed-checkpoint using one gpu, to disable sharding"
        checkpoint.save(curr_step=0, force=True)
        logger.info("Created seed checkpoint")
        return

    checkpoint_loaded = checkpoint.load(step=job_config.checkpoint.load_step)

    if parallel_dims.pp_enabled and not checkpoint_loaded:
        # TODO: fix this by allowing each rank to set their own seed
        logger.warning(
            "Pipeline Parallelism is being used without a seed checkpoint. "
            "All the substages will be initialized with random weights with same RNG state which can affect convergence."
        )

    metric_logger = build_metric_logger(job_config, parallel_dims)
    # ideally we can convert existing MetricLogger into an interface and have wandb_logger as an instance
    # for now, we create it separately for simplicity.
    if job_config.metrics.enable_ibm_wandb:
        try:
            import wandb  # type: ignore
        except ImportError:
            raise ImportError("wandb is enabled in the config but wandb is not installed.")
        if torch.distributed.get_rank() == 0:
            logger.info("wandb is enabled!")
            try:
                wandb.init(
                    project=job_config.metrics.wandb_project_name,
                    dir=job_config.metrics.wandb_dir,
                    resume="allow",
                    id=job_config.metrics.wandb_run_id,
                )
            except wandb.errors.UsageError:
                raise ValueError(
                    "wandb failed to init, did you pass your wandb api key via WANDB_API_KEY?"
                )
            # TODO: dump job_config to wandb. Currently it is not a dataclass so we need some special handling.
            # wandb.config = vars(job_config)

    # plot losses loaded from checkpoint (if any) to TensorBoard
    # NOTE: Loss info after the last log step before checkpoint saving will not be ploted.
    #       This can be avoided by setting checkpoint.interval to be a multiple of metrics.log_freq
    if train_state.step > 0:
        for idx, step in enumerate(train_state.log_steps):
            metrics = {
                "loss_metrics/global_avg_loss": train_state.global_avg_losses[idx],
                "loss_metrics/global_max_loss": train_state.global_max_losses[idx],
            }
            metric_logger.log(metrics, step=step)

    data_iterator = iter(data_loader)

    train_context = utils.get_train_context(
        parallel_dims.loss_parallel_enabled,
        job_config.experimental.enable_compiled_autograd,
    )

    # variables used to keep info for metrics logging
    losses_since_last_log = []
    gnorms_since_last_log = []
    ntokens_since_last_log = 0
    data_loading_times = []
    time_last_log = time.perf_counter()
    device_memory_monitor.reset_peak_stats()

    checkpoint.reset()

    # train loop
    logger.info(
        f"Training starts at step {train_state.step + 1}, "
        f"with local batch size {job_config.training.batch_size}, "
        f"global batch size {job_config.training.batch_size * dp_degree}, "
        f"sequence length {job_config.training.seq_len}, "
        f"total steps {job_config.training.steps} "
        f"(warmup {job_config.training.warmup_steps})"
    )
    with maybe_enable_profiling(
        job_config, global_step=train_state.step
    ) as torch_profiler, maybe_enable_memory_snapshot(
        job_config, global_step=train_state.step
    ) as memory_profiler:
        while train_state.step < job_config.training.steps:
            train_state.step += 1
            train_state.ntokens += job_config.training.batch_size * dp_degree * job_config.training.seq_len
            gc_handler.run(train_state.step)

            # get batch
            data_load_start = time.perf_counter()
            batch = next(data_iterator)
            if len(batch) == 2:
                input_ids, labels = batch
                dataset_stats = batch_size = None
            else:
                # SFT path
                dataset_stats, batch_size, batch_dict = batch
                input_ids, labels = batch_dict["input_ids"], batch_dict["labels"]
                if job_config.training.debug:
                    # TODO: @goon - DELETE
                    # print(f"{input_ids=}")
                    # print(f"{labels=}")
                    # print(f"{dataset_stats=}")
                    # print(f"{batch_size=}")
                    print(f"{input_ids.shape=}")
                    print(f"{input_ids.max()=}")
            ntokens_since_last_log += labels.numel()
            data_loading_times.append(time.perf_counter() - data_load_start)

            input_ids = input_ids.to(device_type)
            labels = labels.to(device_type)

            # apply context parallelism if cp is enabled
            optional_context_parallel_ctx = (
                utils.create_context_parallel_ctx(
                    cp_mesh=world_mesh["cp"],
                    cp_buffers=[input_ids, labels, model.freqs_cis],
                    cp_seq_dims=[1, 1, 0],
                    cp_no_restore_buffers={input_ids, labels},
                )
                if parallel_dims.cp_enabled
                else None
            )

            if parallel_dims.pp_enabled:
                # Pipeline Parallel forward / backward inside step() call
                is_last_stage = pp_mesh.get_local_rank() == pp_mesh.size() - 1

                with train_context(optional_context_parallel_ctx):
                    if pp_mesh.get_local_rank() == 0:
                        pp_schedule.step(input_ids)
                    elif is_last_stage:
                        losses = []
                        pp_schedule.step(target=labels, losses=losses)
                    else:
                        pp_schedule.step()

                # accumulate losses across pipeline microbatches
                loss = (
                    torch.mean(torch.stack(losses))
                    if is_last_stage
                    else torch.Tensor([-1.0])
                )
            else:
                # Non-PP forward / backward
                with train_context(optional_context_parallel_ctx):
                    if job_config.training.debug:
                        # TODO: @goon - DELETE
                        print(
                            f"CP: {parallel_dims.cp_enabled=} {input_ids.shape=} {labels.shape=} {model.freqs_cis.shape=}"
                        )
                        # print(f"CP: {input_ids.to_local().shape=} {labels.to_local().shape=} {model.to_local().freqs_cis.shape=}")
                    pred = model(input_ids)
                    loss = (
                        loss_fn(pred, labels)
                        / job_config.training.gradient_accumulation_steps
                    )
                    # need to free to before bwd to avoid peaking memory
                    del pred
                    loss.backward()

            # sync float8 amaxes and scales
            float8_handler.sync_float8_amax_and_scale_history(model_parts)

            # optimizer step
            checkpoint.maybe_wait_for_staging()
            if train_state.step % job_config.training.gradient_accumulation_steps == 0:
                # clip gradients
                gnorm = utils.clip_grad_norm_(
                    [p for m in model_parts for p in m.parameters()],
                    job_config.training.max_norm,
                    foreach=True,
                    pp_mesh=pp_mesh if parallel_dims.pp_enabled else None,
                )
                gnorms_since_last_log.append(gnorm)
                optimizers.step()
                optimizers.zero_grad()
            lr_schedulers.step(num_steps=job_config.training.steps)

            # calculate float8 dynamic amax/scale for all-parameter for FSDP2
            # it issues a single all-reduce for all parameters at once for better performance
            float8_handler.precompute_float8_dynamic_scale_for_fsdp(model_parts)

            losses_since_last_log.append(loss)

            # log metrics
            if train_state.step % job_config.metrics.log_freq == 0:
                losses = [loss.item() for loss in losses_since_last_log]
                avg_loss, max_loss = (
                    sum(losses)
                    / len(losses)
                    * job_config.training.gradient_accumulation_steps,
                    max(losses),
                )
                gnorms = [gnorm.item() for gnorm in gnorms_since_last_log]
                avg_gnorm = sum(gnorms) / len(gnorms)
                if parallel_dims.dp_enabled:
                    global_avg_loss, global_max_loss = (
                        utils.dist_mean(avg_loss, dp_mesh),
                        utils.dist_max(max_loss, dp_mesh),
                    )
                    global_avg_gnorm = utils.dist_mean(avg_gnorm, dp_mesh)
                else:
                    global_avg_loss, global_max_loss = avg_loss, max_loss
                    global_avg_gnorm = avg_gnorm

                # update train state
                train_state.log_steps.append(train_state.step)
                train_state.global_avg_losses.append(global_avg_loss)
                train_state.global_max_losses.append(global_max_loss)

                time_delta = time.perf_counter() - time_last_log

                # tokens per second per device, abbreviated as tps
                tps = ntokens_since_last_log / (
                    time_delta * parallel_dims.non_data_parallel_size
                )
                # model FLOPS utilization
                # For its definition and calculation, please refer to the PaLM paper:
                # https://arxiv.org/abs/2204.02311
                mfu = 100 * num_flop_per_token * tps / gpu_peak_flops

                time_end_to_end = time_delta / job_config.metrics.log_freq
                time_data_loading = sum(data_loading_times) / len(data_loading_times)
                time_data_loading_pct = 100 * sum(data_loading_times) / time_delta

                device_mem_stats = device_memory_monitor.get_peak_stats()

                metrics = {
                    "loss_metrics/global_avg_loss": global_avg_loss,
                    "loss_metrics/global_max_loss": global_max_loss,
                    "throughput(tps)": tps,
                    "mfu(%)": mfu,
                    "time_metrics/end_to_end(s)": time_end_to_end,
                    "time_metrics/data_loading(s)": time_data_loading,
                    "time_metrics/data_loading(%)": time_data_loading_pct,
                    "memory/max_active(GiB)": device_mem_stats.max_active_gib,
                    "memory/max_active(%)": device_mem_stats.max_active_pct,
                    "memory/max_reserved(GiB)": device_mem_stats.max_reserved_gib,
                    "memory/max_reserved(%)": device_mem_stats.max_reserved_pct,
                    "memory/num_alloc_retries": device_mem_stats.num_alloc_retries,
                    "memory/num_ooms": device_mem_stats.num_ooms,
                }
                metric_logger.log(metrics, step=train_state.step)
                if job_config.metrics.enable_ibm_wandb:
                    if torch.distributed.get_rank() == 0:
                        # for wandb, we track a different set of metrics
                        wandb_metrics = {
                            "loss": global_avg_loss,
                            "gradient norm": global_avg_gnorm,
                            "learning rate": lr_schedulers.schedulers[0].get_last_lr()[0],
                            "num tokens seen": train_state.ntokens,
                            "current throughput": tps,
                            "mfu": mfu,
                        }
                        wandb.log(wandb_metrics, step=train_state.step)

                logger.info(
                    f"{color.cyan}step: {train_state.step:2}  "
                    f"{color.green}loss: {global_avg_loss:7.4f}  "
                    f"{color.yellow}memory: {device_mem_stats.max_reserved_gib:5.2f}GiB"
                    f"({device_mem_stats.max_reserved_pct:.2f}%)  "
                    f"{color.blue}tps: {round(tps):,}  "
                    f"{color.magenta}mfu: {mfu:.2f}%  "
                    f"{color.yellow}gnorm: {global_avg_gnorm}  "
                    f"{color.yellow}lr: {lr_schedulers.schedulers[0].get_last_lr()[0]}{color.reset}"
                )

                losses_since_last_log.clear()
                gnorms_since_last_log.clear()
                ntokens_since_last_log = 0
                data_loading_times.clear()
                time_last_log = time.perf_counter()
                device_memory_monitor.reset_peak_stats()

            checkpoint.save(
                train_state.step, force=(train_state.step == job_config.training.steps)
            )

            # signal the profiler that the next profiling step has started
            if torch_profiler:
                torch_profiler.step()
            if memory_profiler:
                memory_profiler.step()

            # reduce timeout after first train step for faster signal
            # (assuming lazy init and compilation are finished)
            if train_state.step == 1:
                utils.set_pg_timeouts(
                    timeout=timedelta(seconds=job_config.comm.train_timeout_seconds),
                    world_mesh=world_mesh,
                )

    if torch.distributed.get_rank() == 0:
        logger.info("Sleeping 2 seconds for other ranks to complete")
        time.sleep(2)

    metric_logger.close()
    logger.info("Training completed")


if __name__ == "__main__":
    config = JobConfig()
    config.parse_args()
    main(config)
    torch.distributed.destroy_process_group()
